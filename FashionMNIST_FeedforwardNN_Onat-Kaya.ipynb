{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import torch.nn as nn # oop\n",
    "import torch.nn.functional as F # functional, pass parameter every time. \n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.FashionMNIST(\"\", train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.FashionMNIST(\"\", train=False, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# training dataset\n",
    "train_set = torch.utils.data.DataLoader(train, batch_size = 128, shuffle = True)\n",
    "\n",
    "# test dataset\n",
    "test_set = torch.utils.data.DataLoader(test, batch_size = 128, shuffle = True)\n",
    "\n",
    "# defined our batch size as 256. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the cell below, we are going to be building a Feedforward Neural Network (FNN).\n",
    "\n",
    "### In this type of network, the information moves in only one direction—forward—from the input nodes, through the hidden nodes and to the output nodes at last. There are no cycles or loops.\n",
    "\n",
    "### More about FNN: https://www.deeplearningbook.org/contents/mlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (layer3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # inheriting from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__() # running the initialization for nn.Module\n",
    "        \n",
    "        # We are going to building a Feedforward Neural Network (FNN).\n",
    "        \n",
    "        # In this type of network, the information moves in only one direction—forward—from the input nodes,\n",
    "        # through the hidden nodes and to the output nodes at last. There are no cycles or loops.\n",
    "        \n",
    "        # More about FNN: https://www.deeplearningbook.org/contents/mlp.html\n",
    "        \n",
    "        self.layer1 = nn.Linear(784, 128) # first fully connected layer from fully connected (fc) network\n",
    "        # Input: 784 (28*28)\n",
    "        # 128 : number of neurons - in the hidden layer\n",
    "        \n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        # The output of layer1 has to be the input for layer2, which is 128. \n",
    "        \n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        \n",
    "        self.layer4 = nn.Linear(64, 10)\n",
    "        # This our output layer. Output is set as 10 since we have 10 labels.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "       # We have used ReLu as our activation function. \n",
    "        \n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.layer4(x) # We don't need activation function in the output layer.\n",
    "        \n",
    "        # In the output, we want a probability distribution. So, we use soft-max for that.\n",
    "\n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "\n",
    "my_net = Net() # creating a neural network object. \n",
    "print(my_net) # printing a summary of our nn object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(my_net.parameters(), lr = 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent (SGD) to update network weights and biases based in training data.\n",
    "\n",
    "### Read about Adam more at: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "\n",
    "### lr: learning rate. dictates size of the step which the optimizer will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 1.7585844993591309\n",
      "Epoch: 2 - Loss: 1.7413250207901\n",
      "Epoch: 3 - Loss: 1.7327237129211426\n",
      "Epoch: 4 - Loss: 1.7772029638290405\n",
      "Epoch: 5 - Loss: 1.7265501022338867\n",
      "Epoch: 6 - Loss: 1.6875442266464233\n",
      "Epoch: 7 - Loss: 1.637118935585022\n",
      "Epoch: 8 - Loss: 1.6576181650161743\n",
      "Epoch: 9 - Loss: 1.6405973434448242\n",
      "Epoch: 10 - Loss: 1.617505669593811\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in train_set:\n",
    "                    \n",
    "        x, y = data\n",
    "        \n",
    "        my_net.zero_grad()\n",
    "        # Sets the gradients of all optimized torch.Tensor s to zero.\n",
    "        \n",
    "        # for every mini-batch during the training phase, we typically want to explicitly set\n",
    "        # the gradients to zero before starting to do backpropragation.\n",
    "        # (i.e. updating weights and biases)\n",
    "\n",
    "        x = x.view(-1, 28*28) # reshaping.\n",
    "\n",
    "        output = my_net(x)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        loss_calc = loss(output, y)\n",
    "        \n",
    "        # output: The result we found; y: What it should be (target).\n",
    "        \n",
    "        # In this case, CrossEntropyLoss is used since this is a multi-class classification problem.\n",
    "        # More about loss functions: https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/\n",
    "        \n",
    "        \n",
    "        loss_calc.backward()\n",
    "        # gradients are being computed\n",
    "        \n",
    "        optimizer.step()\n",
    "        # calculated gradient values are plugged in. \n",
    "        \n",
    "    print(\"Epoch:\", epoch+1, \"- Loss:\", loss_calc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  84.97\n"
     ]
    }
   ],
   "source": [
    "correct_cases = 0\n",
    "total_cases = 0\n",
    "\n",
    "# Let's test the network on test data.\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for data in test_set:\n",
    "        \n",
    "        x, y = data\n",
    "        \n",
    "        output = my_net(x.view(-1,28*28)) # reshaping our batch.   \n",
    "    \n",
    "        # let's check whether the results we found from \"output\" match with the target.\n",
    "        for index, values in enumerate(output):\n",
    "            \n",
    "            if torch.argmax(values) == y[index]:\n",
    "                correct_cases += 1\n",
    "                \n",
    "            total_cases += 1\n",
    "\n",
    "print(\"Test Accuracy: \", round(correct_cases*100/total_cases, 2)) # print the test accuracy, round it up to 2 decimals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For better accuracy adjustments can be made with hyperparameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
